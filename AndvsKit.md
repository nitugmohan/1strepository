# Kitchin vs. Anderson

### Kitchin
The scientific process is one that has been used and trusted for many decades of science. Where Anderson says that “correlation is enough” and that “we can analyze data without hypotheses,” Kitchin describes that these types of analysis “are disruptive innovations” that reconfigure” how research is conducted. Anderson’s view on Big Data is proved fallacious by four reasons that Kitchin describes and well as examples he provided in business.   

When looking at retail chains, companies often use unnoticed correlations in what customers were buying to increase profits. This simply looked at relationships that were not visible before; there was no hypothesis, no experimentation, and no trial. These types of data usage are used to “predict” the world, rather than “understand” it.   

Kitchin goes further to explain why this type of thinking is “fallacious.” First, he explains that this data is based on sample data and is subject to human bias. This data is noticed, used, and analyzed with a certain outlook in mind. It is not looked at with a “neutral and objective way.” The empiricist theory allows people to believe that data can hold itself  and eliminate human bias which is wrong. As Anderson puts it, this data is void of bias which is not true in the case of sampling bias. Biases can corrupt conclusions made without hypotheses and bring about dire consequences. It can almost rescue the data to noise. 
Second, this data is brought with a specific goal in mind. The algorithms that were used weretests for their “validity or their “veracity.” New analytical methods may find correlations and coincidence in the data, but they might answer questions that were never asked and show new relationships that are unnecessary to the goal.   
Third, we bring reason and context to data. Even further, correlation between data sets can be completely random and purely coincidental. It is almost comparable to a stab in the dark; the computer is simply looking for a correlation, but one needs a hypothesis to build data and learn from it. This can produce even more fallacious arguments with the data as evidence. This data science produces a conclusion without context. By ignoring this context, the conclusions become inaccurate. The inductive approach has fallacies.  Fourth, data is a lot to process and understand. By using simple terms and not using work that has been done previously, these conclusions prove unhelpful when looking at broader topics. This empirical frame is useful for businesses, such as Walmart who can generate massive amounts of data and use it for profit. It can simplify the approach to Big Data, but it requires context and hypothesis to draw out valuable and accurate conclusions.   

### Anderson
"All models are wrong, but some are useful” is the argument Anderson is proving. As time has gone on, the massive amounts of data we hold has grown into such a size we can no longer store it on a physical device such as a hard drive. Since this data is so gargantuan in size, we have to look at it with math and statistics first, before assigning any context or meaning to it. Using the method of hypothesize, model, and test is no longer useful with such large amounts of data. Even in scientific fields such as physics and biology, we drift away from being able to model findings. Disciplines that are starved from data drift to theory, but Big Data can bring them back and use that data to prove and find conclusions. At this point, using theories and assigning meaning to data is either too expensive (colliders for example) or too far out of our physical reach to make with physical models. Using statistics and mathematical properties, we can grasp the data and understand its usage.   

A successful example of this, is when Anderson mentions Google’s advertising and how they go about using the data they generate. Simply using data and analytics allowed Google to correlate ads to specific areas without the actual knowledge of that area.  Using theory for human behavior is always questionable and is constantly changing and evolving, but by using mathematics and data, it provides the answer for itself. Another aspect he looks at with Google is Google Translate. Since analysis is not required, they are able to translate languages with no connection without actual knowledge of the language structures. Anderson specifically looks at Klingon, a language from Star Trek, and Farsi, a modern Persian language. With this view of data analysis, Google has been able to do multiple feats in a timely, efficient, and accurate way.   

Another such example is with J. Craig Venter. By using shotgun sequencing, Venter was able to sequence the ocean and then the air. By doing so he was able to  prove the existence of “thousands of previously known species.” Though he cannot provide information about the species, he is able to provide evidence that it existed, “a statistical blip.” By using this data in such a way though, he was able to advance biology. Looking at this in a timely manner, if Venter spent time to create a hypothesis, run tests, and look at context, he wouldn’t have been able to find these new species. Just by using the data and correlation, he was able to conclude the existence of an undiscovered species. This type of efficiency and timeliness could be a valuable benefit moving forward with Big Data.   

In this age of such massive amounts of data, “correlation supersedes causation.”   



